{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low-Level API [under construction for v7.0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../_static/images/banner/pipes.png\" class=\"banner-photo\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**item** | type | None | text\n",
    "**item** | type | None | text\n",
    "**item** | type | None | text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object-Relational Model (ORM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Low-Level API is an *object-relational model* for machine learning. Each class in the [ORM](http://docs.peewee-orm.com/en/latest/peewee/models.html) maps to a table in a SQLite database that serves as a machine learning *metastore*. \n",
    "\n",
    "The real power lies in the relationships between these objects (e.g. `Label`→`Splitset`←`Feature` and `Queue`→`Job`→`Predictor`→`Prediction`), which enable us to construct rule-base protocols for various types of data and analysis.\n",
    "\n",
    "Goobye, *X_train, y_test*. Hello, object-oriented machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. `Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Datasets](../_static/images/api/dimensions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dataset` class provides the following subclasses for working with different types of data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type | Dimensionality | Supported Formats | Format (if ingested)\n",
    "--- | --- | --- | ---\n",
    "**Tabular** | 2D | Files (Parquet, CSV, TSV, Parquet) / Pandas DataFrame (in-memory) | Parquet\n",
    "**Sequence** | 3D | NumPy (in-memory ndarray, npy file) | npy \n",
    "**Image** | 4D | NumPy (in-memory ndarray, npy file) / Pillow-supported formats | npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The names are merely suggestive, as the primary purpose of these subclasses is to provide a way to register data of known dimensionality.  For example, a practitioner could ingest many uni-channel/ grayscale images as a 3D Sequence Dataset instead of a multi-channel 4D Image Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Why not 2D NumPy?* The `Dataset.Tabular` class is intended for strict, column-specific dtypes and Parquet persistence upon ingestion. In practice, this conflicted too often with NumPy's array-wide dtyping. We use the best tools for the job (df/pq for 2D) and (array/npy for ND)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a. Register"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Most of the Dataset registration methods share these arguments:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Description\n",
    "--- | ---\n",
    "**ingest** | Determines if raw data is either stored directly inside the metastore or remains on disk to be accessed via path/url. *In-memory* data like DataFrames and ndarrays must be ingested. Whereas *file-based* data like Parquet, NPY, Image folders/urls may remain remote. Regardless of whether or not the raw data is ingested, metadata is always derived from it by parsing: 2D via DataFrame and N-D via ndarray.\n",
    "**rename_columns** | Useful for assigning column names to arrays or delimited files that would otherwise be unnamed. `len(rename_columns)` must match the number of columns in the raw data. Normally, an int-based range is assigned to unnamed columns. In this case, AIQC converts each column name to a string e.g. '1' during the registration process.\n",
    "**retype** | Change the dtype of data using [np.types](https://numpy.org/doc/stable/user/basics.types.html). All Dataset subclasses support mass typing via `np.type`/ `str(np.type)`. Only the Tabular subclass supports inidividual column retyping via `dict(column=str(np.type))`. If `rename_columns` is used in conjuction with `retype=dict()`, then each `dict['column']` key must match its counterpart in rename_columns.\n",
    "**description** | What information does this dataset contain? What is unique about this dataset/ version -- did you edit the raw data, add rows, or change column names/ dtypes?\n",
    "**name** | Triggers dataset *versioning*. Datasets that share a name will be assigned an auto-incrementing `version:int` number provided that they are not duplicates of each other based on a `sha256_hexdigest:str` hash. If you try to create an exact duplicate, it will warn you and `return` the matching duplicate instead of creating a new entity. This behavior makes it easy to rerun pipelines where Datasets are created inline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ingestion provides the following benefits, especially for entry-level users:*\n",
    "\n",
    "- Persist in-memory datasets (Pandas DataFrames, NumPy ndarrays).\n",
    "- Keeps data coupled with the experiment in the portable SQLite file.\n",
    "- Provides a more immutable and out-of-the-way storage location in comparison to a laptop file system.\n",
    "- Encourages preserving tabular dtypes with the ecosystem-friendly Parquet format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Why would I avoid ingestion?*\n",
    "\n",
    "- Happy with where the original data lives: e.g. S3 bucket.\n",
    "- Don't want to duplicate the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *sha256?* -- It's the one-way hash algorithm that GitHub aspires to upgrade to. AIQC runs it on compressed data because it's easier and probably less-error prone than intercepting the bytes of the *fastparquet* intermediary tables before appending the Parquet magic bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Is SQLite a legitimate datastore?* -- In many cases, SQLite queries are faster than accessing data via a filesystem. It's a stable, 22 year-old technology that serves as the default database for iOS e.g. Apple Photos. AIQC uses it store raw data in byte format as a BlobField. I've stored tens-of-thousands of files in it over several years and never experienced corruption. Keep in mind that AWS S3 is blob store, and the Microsoft equivalent service is literally called Azure *Blob* Storage. The max size of a BlobField is 2GB, so ~20GB after compression. Either way, the goal of machine learning isn't to record the entire population within the weights of a neural network, it's to find subsets that are representative of the broader population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1ai. `Dataset.Tabular`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the ways practitioners can use this 2D structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||||\n",
    "|---|---|---|\n",
    "Multiple subjects (1 row per sample) | * | Multi-variate 1D (1 col per attribute)\n",
    "Single subject (1 row per timestamp) | * | Multi-variate 1D (1 col per attribute)\n",
    "Multiple subjects (1 row per timestamp) | * | Uni-variate 0D (1 col per sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tabular datasets may contain both features and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### └── `Dataset.Tabular.from_df()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "```python\n",
    "dataset = Dataset.Tabular.from_df(\n",
    "    dataframe\n",
    "    , rename_columns\n",
    "    , retype\n",
    "    , description\n",
    "    , name\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**df** | DataFrame | Required | [pd.DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#pandas-dataframe) with int-based single index. DataFrames are always ingested.\n",
    "**rename_columns** | list[str]  | None | See [Registration](#1a.-Registration)\n",
    "**retype** | np.type / dict(column:np.type) | None | See [Registration](#1a.-Registration)\n",
    "**description** | str | None | See [Registration](#1a.-Registration)\n",
    "**name** | str | None | See [Registration](#1a.-Registration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### └── `Dataset.Tabular.from_path()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Dataset.Tabular.from_path(\n",
    "    file_path\n",
    "    , ingest\n",
    "    , rename_columns\n",
    "    , retype\n",
    "    , header\n",
    "    , description\n",
    "    , name\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**file_path** | str | Required | Parsed based on how the file name ends (.parquet, .tsv, .csv)\n",
    "**ingest** | bool | True | See [Registration](#1a.-Registration). Defaults to True because I don't want to rely on CSV files as a source of truth for dtypes, and compression works great in Parquet.\n",
    "**rename_columns** | list[str]  | None | See [Registration](#1a.-Registration)\n",
    "**retype** | np.type / dict(column:np.type) | None | See [Registration](#1a.-Registration)\n",
    "**header** | object | None | See [Registration](#1a.-Registration) \n",
    "**description** | str | None | See [Registration](#1a.-Registration)\n",
    "**name** | str | None | See [Registration](#1a.-Registration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1aii. `Dataset.Sequence`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the ways practitioners can use this 3D structure:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||||\n",
    "|---|---|---|\n",
    "Single subject (1 patient) | * | Multiple 2D sequences\n",
    "Multiple subjects | * | Single 2D sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sequence datasets are somewhat multi-modal in that, in order to perform supervised learning on them, they must eventually be paired with a `Dataset.Tabular` that acts as its `Label`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### └── `Dataset.Sequence.from_numpy()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Dataset.Sequence.from_numpy(\n",
    "    arr3D_or_npyPath\n",
    "    , ingest\n",
    "    , rename_columns\n",
    "    , retype        \n",
    "    , description\n",
    "    , name           \n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**arr3D_or_npyPath** | object / str | Required | 3D array in the form of either an [ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html) or [npy](https://numpy.org/doc/stable/reference/generated/numpy.save.html) file path\n",
    "**ingest** | bool | None | See [Registration](#1a.-Registration). If left blank, ndarrays will be ingested and npy will not. Errors if ndarray and False.\n",
    "**rename_columns** | list[str]  | None | See [Registration](#1a.-Registration)\n",
    "**retype** | np.type / dict(column:np.type) | None | See [Registration](#1a.-Registration)\n",
    "**description** | str | None | See [Registration](#1a.-Registration)\n",
    "**name** | str | None | See [Registration](#1a.-Registration) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1aiii. `Dataset.Image`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the ways you can practitioners this 4D structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||||\n",
    "|---|---|---|\n",
    "Single subject (1 patient) | * | Multiple 3D images\n",
    "Multiple subjects | * | Single 3D image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Users can ingest 4D data using either:\n",
    "- [The Pillow library, which supports various formats](pillow.readthedocs.io/en/stable/handbook/image-file-formats.html)\n",
    "- Or NumPy arrays as a simple alternative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Image datasets are somewhat multi-modal in that, in order to perform supervised learning on them, they must eventually be paired with a `Dataset.Tabular` that acts as its `Label`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### └── `Dataset.Image.from_numpy()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Dataset.Image.from_numpy(\n",
    "    arr4D_or_npyPath\n",
    "    , ingest\n",
    "    , rename_columns\n",
    "    , retype        \n",
    "    , description\n",
    "    , name       \n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**arr4D_or_npyPath** | object / str | Required | 4D array in the form of either an [ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html) or [npy](https://numpy.org/doc/stable/reference/generated/numpy.save.html) file path\n",
    "**ingest** | bool | None | See [Registration](#1a.-Registration). If left blank, ndarrays will be ingested and npy will not. Errors if ndarray and False.\n",
    "**rename_columns** | list[str]  | None | See [Registration](#1a.-Registration) \n",
    "**retype** | np.type / dict(column:np.type) | None | See [Registration](#1a.-Registration) \n",
    "**description** | str | None | See [Registration](#1a.-Registration) \n",
    "**name** | str | None | See [Registration](#1a.-Registration) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### └── `Dataset.Image.from_folder()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Dataset.Image.from_folder(\n",
    "    folder_path\n",
    "    , ingest\n",
    "    , rename_columns\n",
    "    , retype\n",
    "    , description\n",
    "    , name\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**folder_path** | str | Required | Folder of images to be ingested via Pillow. All images must be cropped to the same dimensions ahead of time.\n",
    "**ingest** | bool | False | See [Registration](#1a.-Registration) \n",
    "**rename_columns** | list[str]  | None | See [Registration](#1a.-Registration) \n",
    "**retype** | np.type / dict(column:np.type) | None | See [Registration](#1a.-Registration) \n",
    "**description** | str | None | See [Registration](#1a.-Registration) \n",
    "**name** | str | None | See [Registration](#1a.-Registration) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### └── `Dataset.Image.from_urls()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Dataset.Image.from_urls(\n",
    "    urls\n",
    "    , source_path\n",
    "    , ingest\n",
    "    , rename_columns\n",
    "    , retype\n",
    "    , description\n",
    "    , name\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**urls** | list(str) | Required | URLs that point to an image to be ingested via Pillow. All images must be cropped to the same dimensions ahead of time.\n",
    "**source_path** | str  | None | Optionally record a shared directory, bucket, or FTP site where images are stored. The backend won't use this information for anything. |\n",
    "**ingest** | bool | False | See [Registration](#1a.-Registration)\n",
    "**rename_columns** | list[str]  | None | See [Registration](#1a.-Registration) \n",
    "**retype** | np.type / dict(column:np.type) | None | See [Registration](#1a.-Registration) \n",
    "**description** | str | None | See [Registration](#1a.-Registration) \n",
    "**name** | str | None | See [Registration](#1a.-Registration) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Fetch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following methods are exposed to end-users in case they want to inspect the data that they have ingested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### └── `Dataset.to_arr()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**id** | int | None | The id of the Dataset\n",
    "**columns** | list(str) | None | If left blank, includes all columns\n",
    "**samples** | list(int) | None | If left blank, includes all samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Subclass | Returns\n",
    "--- | ---\n",
    "Tabular | ndarray.ndim==2\n",
    "Sequence | ndarray.ndim==3\n",
    "Image | ndarray.ndim==4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### └── `Dataset.to_df()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**id** | int | None | The id of the Dataset\n",
    "**columns** | list(str) | None | If left blank, includes all columns\n",
    "**samples** | list(int) | None | If left blank, includes all samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Subclass | Returns\n",
    "--- | ---\n",
    "Tabular | DataFrame\n",
    "Sequence | list(DataFrame)\n",
    "Image | list(list(DataFrame))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### └── `Dataset.to_pillow()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**id** | int | None | The id of the Dataset\n",
    "**samples** | list(int) | None | If left blank, includes all samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subclass | Returns\n",
    "--- | ---\n",
    "Image | list(PIL.Image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### └── `Dataset.get_dtypes()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of how the initial `Dataset.dtype` was formatted [e.g. single np.type / str(np.type) / dict(column=np.type)], this function intentionally returns dtype of each column in `dict(column=str(np.type)` format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**id** | int | None | The id of the Dataset\n",
    "**columns** | list(str) | None | If left blank, includes all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. `Feature`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determines the columns that will be used as predictive features during training. Columns is always the last dimension `shape[-1]` of a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### └── `Feature.from_dataset()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Feature.from_dataset(\n",
    "    dataset_id\n",
    "    , include_columns\n",
    "    , exclude_columns\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**dataset_id** | int | Required | `Dataset.id` from which you want to derive `Dataset.columns`.\n",
    "**include_columns**  | list(str) | None | Specify columns that *will* be included in the Feature. All columns that are not specified will *not* be included.\n",
    "**exclude_columns** | list(str) | None | Specify columns that will *not* be included in the Feature. All columns that are not specified *will* be included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If neither `include_columns` nor `exclude_columns` is defined, then all columns will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Both `include_columns` and `exclude_columns` cannot be used at the same time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Fetch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theses methods wrap Dataset's [fetch](#1b.-Fetch) methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method | Arguments | Returns\n",
    "--- | --- | ---\n",
    "**to_arr()** | columns:list(str)=Feature.columns, samples:list(int)=None | ndarray 2D / 3D / 4D\n",
    "**to_df()** | columns:list(str)=Feature.columns, samples:list(int)=None | df / list(df) / list(list(df))\n",
    "**get_dtypes()** | columns:list(str)=Feature.columns | dict(column=str(np.type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. `Label`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determines the column(s) that will be used as a target during supervised analysis. Do no create a Label if you intend to conduct unsupervised/ self-supervised analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### └── `Label.from_dataset()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Label.from_dataset(\n",
    "    dataset_id\n",
    "    , columns\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**dataset_id** | int | Required | `Dataset.id` from which you want to derive `Dataset.columns`. Only Tabular Datasets may be used as a Label.\n",
    "**columns**  | list(str) | None | Specify columns that *will* be included in the Label. If left blank, defaults to all columns. If more than 1 column is provided, then the data in those columns must be in One-Hot Encoded (OHE) format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Fetch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theses methods wrap Dataset's [fetch](#1b.-Fetch) methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method | Arguments | Returns\n",
    "--- | --- | ---\n",
    "**to_arr()** | columns:list(str)=Label.columns, samples:list(int)=None | ndarray 2D / 3D / 4D\n",
    "**to_df()** | columns:list(str)=Label.columns, samples:list(int)=None | df / list(df) / list(list(df))\n",
    "**get_dtypes()** | columns:list(str)=Label.columns | dict(column=str(np.type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interpolate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have continuous columns with missing data in a time series, then interpolation allows you to fill in those blanks mathematically. It does so by fitting a curve to each column. If you don't have time series data then you do not need interpolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolation is the first preprocessor because you need to fill in blanks prior to encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `pandas.DataFrame.interpolate`\n",
    "> \n",
    "> https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.interpolate.html\n",
    "> \n",
    "> Is utilized due to its ease of use, variety of methods, and **support of sparse indices**. However, it does not follow the `fit/transform` pattern like many of the class-based sklearn preprocessors, so the interpolated training data is concatenated with the evalaution split during the interpolation of evaluation splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the default settings if `interpolate_kwargs=None`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "interpolate_kwargs = dict(\n",
    "    method            = 'spline'\n",
    "    , limit_direction = 'both'\n",
    "    , limit_area      = None\n",
    "    , axis            = 0\n",
    "    , order           = 1\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Type | Approach\n",
    "--- | ---\n",
    "**Tabular** | Unlike encoders, there is no `fit` object. So first the training data rows are interpolated independently. Then, when it comes time to interpolate other splits like validation, the training data is included in the sequence to be interpolated.\n",
    "**Sequence** | Interpolation is ran on each 2D sequence separately\n",
    "**Image** | Interpolation is ran on each 2D channel separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. `LabelInterpolater`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label is intended for a single column, so only 1 Interpolater will be used during `Label.preprocess()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### └── `LabelInterpolater.from_label()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "LabelInterpolater.from_label(\n",
    "    label_id\n",
    "    , process_separately\n",
    "    , interpolate_kwargs\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**label_id** | int | Required | Points to the `Label.columns` to be interpolated\n",
    "**process_separately** | bool | True | Used to restrict the fit to the training data, this may be flipped to `False`. However, doing so causes data leakage.\n",
    "**interpolate_kwargs** | dict | None | The `interpolate_kwargs:dict=None` object is what gets passed to Pandas interpolation. In my experience, `method=spline` produces the best results. However, if either (a) spline fails to fit to your data, or (b) you know that your pattern is linear - then try `method=linear`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. `FeatureInterpolater`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For *multivariate datasets* there may be several columns/ dtypes that have completely different patterns/ curves to fit. So we can specify multiple FeatureInterpolaters, and give them column/dtype filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### └── `FeatureInterpolater.from_feature()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "FeatureInterpolater.from_feature(\n",
    "    feature_id\n",
    "    , process_separately\n",
    "    , interpolate_kwargs\n",
    "    , dtypes\n",
    "    , columns\n",
    "    , verbose\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**feature_id** | int | Required | Points to the `Feature.columns` to be interpolated\n",
    "**process_separately** | bool | True | Used to restrict the fit to the training data, this may be flipped to `False`. However, doing so causes data leakage.\n",
    "**interpolate_kwargs** | dict | None | The `interpolate_kwargs:dict=None` object is what gets passed to Pandas interpolation. In my experience, `method=spline` produces the best results. However, if either (a) spline fails to fit to your data, or (b) you know that your pattern is linear - then try `method=linear`.\n",
    "**dtypes** | type | None | The dtypes to include\n",
    "**columns** | type | None | The columns to include. Errors if any of the columns were already included by dtypes.\n",
    "**verbose** | bool | True | If True, messages will be printed about the status of the interpolaters as they attempt to fit on the filtered columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Encode Features & Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain algorithms either (a) require features and/ or labels formatted a certain way, or (b) perform significantly better when their values are normalized. For example:\n",
    "\n",
    "* Scaling continuous features from (-1 to 1) or (0.0 to 1.0). Or transforming them to resemble a more Gaussian distribution.\n",
    "* Converting ordinal or categorical string data `[dog, cat, fish]` into one-hot encoded format `[[1,0,0][0,1,0][0,0,1]]`.\n",
    "\n",
    "There are two phases of encoding:\n",
    "1. `fit` - where the encoder learns about the values of the samples made available to it. Ideally, you only want to `fit` aka learn from your training split so that you are not *\"leaking\"* information from your validation and test spits into your encoder!\n",
    "2. `transform` - where the encoder transforms all of the samples in the population.\n",
    "\n",
    "AIQC has solved the following challenges related to encoding:\n",
    "\n",
    "* How does one dynamically `fit` on only the training samples in advanced scenarios like cross-validation where a different fold is used for validation each time?\n",
    "\n",
    "* For certain encoders, especially categorical ones, there is arguably no leakage. If an encoder is arbitrarilly assigning values/ tags to a sample through a process that is not aggregate-informed, then the information that is reveal to the `fit` is largely irrelevant. As an analogy, if we are examining swan color and all of a sudden there is a black swan... it's clearly not white, so slap a non-white label on it and move on. In fact, the prediction process and performance metric calucatlion may fail if it doesn't know how to handle the previously unseen category.\n",
    "\n",
    "* Certain encoders only accept certain dtypes. Certain encoders only accept certain dimensionality (e.g. 1D, 2D, 3D) or shape patterns (odd-by-odd square). Unfortunately, there is not much uniformity here.\n",
    "\n",
    "* Certain encoders output extraneous objects that don't work with deep learning libraries.\n",
    "\n",
    "> Only `sklearn.preprocessing` methods are officially supported, but we have experimented with `sklearn.feature_extraction.text.CountVectorizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Encode labels with `LabelCoder`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the name \"LabelEncoder\" is occupied by `sklearn.preprocessing.LabelEncoder`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you cannot encode Labels if your `Splitset` does not have labels in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process is straightforward. You provide an instantiated encoder [e.g. `StandardScaler()` not `StandardScaler`], and then AIQC will:\n",
    "\n",
    "* Verify that the encoder works with your `Label`'s dtype, sample values, and figure out what dimensionality it needs in order to succeed.\n",
    "\n",
    "* Automatically correct the attributes of your encoder to smooth out any common errors they would cause. For example, preventing the output of a sparse scipy matrix.\n",
    "\n",
    "* Determine whether the encoder should be `fit` either (a) exclusively on the train split, or (b) if it is not prone to leakage, inclusively on the entire dataset thereby reducing the chance of errors arising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a `LabelCoder`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AIQC only supports the uppercase `sklearn.preprocessing` methods (e.g. `RobustScaler`, but not `robust_scale`) because the lowercase methods do not separate the `fit` and `transform` steps. FYI, most of the uppercase methods have a combined `fit_transform` method if you need them. \n",
    "\n",
    "> https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelcoder = LabelCoder.from_label(\n",
    "    label_id=label.id, sklearn_preprocess=OneHotEncoder(sparse=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method is used behind the scenes to fetch the most recently create LabelCoder for your Label when it comes time to encode data during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Encode Features sequentially with `FeatureCoders`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `FeatureCoder` has the same validation process as the `LabelCoder`. However, it is not without its own challenges:\n",
    "\n",
    "* We want to be able to apply different encoders to columns of different dtypes.\n",
    "\n",
    "* Additionally, even within the same dtype (e.g. float/ continuous), different distributions call for different encoders.\n",
    "\n",
    "* Commonly used encoders such a `OneHotEncoder` can ouput multiple columns from a single column input. Therefore, the *shape* of the features can change during encoding.\n",
    "\n",
    "* And finally, throughout this entire process, we need to avoid data leakage.\n",
    "\n",
    "For these reasons, `FeatureCoder`'s are applied sequentially; in an ordered chain, one after the other. After an encoder is applied, its columns are removed from the raw feature and placed into an intermediary cache specific to each split/ fold. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering feature columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filtering mode is either:\n",
    "\n",
    "* Inclusive (`include=True`) encode columns that match the filter.\n",
    "\n",
    "* Exclusive (`include=False`) encode columns outside of the filter.\n",
    "\n",
    "Then you can select:\n",
    "\n",
    "1. An optional list of `dtypes`.\n",
    "\n",
    "2. An optional list of `columns` name.\n",
    "\n",
    "  * The column filter is applied after the dtype filter. \n",
    "  \n",
    "> You can create a filter for all columns by setting `include=False` and then seting both `dtypes` and `columns` to `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After submitting your encoder, if `verbose=True` is enabled:\n",
    "* The validation rules help determine why it may have failed.\n",
    "* The print statements help determine which columns your current filter matched, and which raw columns remain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "___/ featurecoder_index: 0 \\_________\n",
      "\n",
      "=> The column(s) below matched your filter(s) featurecoder filters.\n",
      "\n",
      "['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "\n",
      "=> Done. All feature column(s) have featurecoder(s) associated with them.\n",
      "No more FeatureCoders can be added to this Encoderset.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FeatureCoder = FeatureCoder.from_feature(\n",
    "    feature_id = feature.id\n",
    "    , sklearn_preprocess = PowerTransformer(method='yeo-johnson', copy=False)\n",
    "    , include = True\n",
    "    , dtypes = ['float64']\n",
    "    , columns = None\n",
    "    , verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also view this information via the following attributes: `matching_columns`, `leftover_dtypes`, and `leftover_columns`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reshape Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with architectures that are highly dimensional such convolutional and recurrent networks (Conv1D, Conv2D, Conv3D / ConvLSTM1D, ConvLSTM2D, ConvLSTM3D), you'll often find yourself needing to reshape data to fit a layer's required input shape. \n",
    "\n",
    "AIQC ingestion & preprocessing favors a *\"channels_first\"* (samples, channels, rows, columns) approach as opposed to *\"channels_last\"* (samples, rows, columns, channels).\n",
    "\n",
    "- *Reducing unused dimensions* - When working with grayscale/ single channel images (1 channel, 25 rows, 25 columns) there is no sense using Conv2D just to handle that 1 channel.\n",
    "- *Adding wrapper dimensions* - Perhaps your data is a fit for ConvLSTM1D, but that layer is only supported in the nightly TensorFlow build so you want to add a wrapper dimension in order to use the production-ready ConvLSTM2D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is difficult do this on the fly during training (aka after the fact) because you need to: add reshaping layers/ views to your model, intercept and reshape the data in your post-processing functions, and, by this point, the data is in a variety of tensor formats. It's also more efficient to do this wrangling once up front rather than repeatedly on every training run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reshape_indices` argument accepts a tuple for rearranging indices in your order of choosing. Behind the scenes, it will use `np.reshape()` to rearrange the data at the end of your preprocessing pipeline. How the element is handled in that tuple is determined by its type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`feature.make_featureshaper(reshape_indices:tuple)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# source code from the end of `feature.preprocess()`\n",
    "current_shape = feature_array.shape\n",
    "\n",
    "new shape = []\n",
    "for i in featureshaper.reshape_indices:\n",
    "    if (type(i) == int):\n",
    "        new_shape.append(current_shape[i])\n",
    "    elif (type(i) == str):\n",
    "        new_shape.append(int(i))\n",
    "    elif (type(i)== tuple):\n",
    "        indices = [current_shape[idx] for idx in i]\n",
    "        new_shape.append(math.prod(indices))\n",
    "new_shape = tuple(new_shape)\n",
    "            \n",
    "feature_array = feature_array.reshape(new_shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Warning:* if your model is unsupervised (aka generative or self-supervised), then it must output data in *\"column (aka width) last\"* shape. Otherwise, automated column decoding will be applied along the wrong dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping by Index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have a 4D feature consisting of 3D images `(samples * color channels * rows * columns)`. Our image is B&W, so we want to get rid of the single color channel. So we want to drop the dimension at the shape index `1`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "reshape_indices = (0,2,3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we have wrangled ourselves a 3D feature consisting of 2D images `(samples * rows * columns)`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping Explicitly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if the dimensions we want cannot be expressed by rearranging the existing indices? You might have been wondering why `str` appeared in the loop above. If you define a string-based number, then that number will be used as directly as the value at that position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if I wanted to add an extra wrapper dimension to my data to serve as a single color channel, I would simply do:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "reshape_indices = (0,'1',1,2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplicative Reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you need to stack/nest dimensions. This requires multiplying one shape index by another. For example, if I have a 3 separate hours worth of data and I want to treat it as 180 minutes, then I need to go from a shape of (3 hours * 60 minutes) to (180 minutes). Just provide the shape indices that you want to multiply in a `tuple` like so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "reshape_indices = ((0,1), 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Window Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![window_dimensions](../_static/images/api/window_dimensions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Window facilitates sliding windows for a time series Feature. It does not apply to Labels. This is used for unsupervised (aka self-supervised) time series walk-forward forecasting.\n",
    "\n",
    "As seen above, no matter what dimensionality the original data has, it will be windowed along the samples (first) dimension. `size_window` determine how many timepoints are included in a window, and `size_shift` determines how many timepoints to slide over before defining a new window. \n",
    "\n",
    "> For example, if we want to be able to *predict the next 7 days* worth of weather *using the past 21 days* of weather, then our `size_window=21` and our `size_shift=7`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![window_samples](../_static/images/api/stratify_window.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After data is windowed, its dimensionality increases by 1. Why? Well, originally we had a *single time series*. However, if we window that data, then we have *many time series subsets*.\n",
    "\n",
    "This means that the windows now act as the sample dimension, which is important for stratification. Therefore, Window must be created prior to Splitset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Windows](../_static/images/api/sliding_windows.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a walk-forward analysis, we learn about the future by looking at the past. So we need 2 sets of windows:\n",
    "\n",
    "- *Unshifted windows* (orange in diagram above): represent the past and serves as the features we learn from\n",
    "- *Shifted windows* (green in diagram above): represent the future and serves as the target we predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, when conducting inference, we are trying to predict the shifted windows not learn from them. So we don't need to record any shifted windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7a. Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### └── `Window.from_feature()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Window.from_feature(\n",
    "    feature_id\n",
    "    , size_window\n",
    "    , size_shift\n",
    "    , record_shifted\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**dataset_id** | int | Required | `Feature.id` from which you want to derive windows.\n",
    "**size_window**  | int | Required | The number of timesteps to include in a window.\n",
    "**size_shift**  | int | Required | The number of timesteps to shift forward.\n",
    "**record_shifted**  | bool | True | Whether or not we want to keep a shifted set of windows around. During pure inference, this is False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. `Splitset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used for sample stratification. Reference [Stratification](https://aiqc.readthedocs.io/en/latest/pages/explainer.html) section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split | Description\n",
    "--- | ---\n",
    "**train** | The samples that the model will be trained upon. Later, we’ll see how we can make *cross-folds from our training split*. Unsupervised learning will only have a training split.\n",
    "**validation** (optional) | The samples used for training evaluation. Ensures that the test set is not revealed to the model during training.\n",
    "**test** (optional) | The samples the model has never seen during training. Used to assess how well the model will perform on unobserved, natural data when it is applied in the real world aka how generalizable it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Because Splitset groups together all of the data wrangling entities (Features, Label, Folds) it essentially represents a *Pipeline*, which is why it bears the name Pipeline in the High-Level API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8a. Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### └── `Splitset.make()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Splitset.make(\n",
    "    feature_ids\n",
    "    , label_id\n",
    "    , size_test\n",
    "    , size_validation\n",
    "    , bin_count\n",
    "    , fold_count\n",
    "    , unsupervised_stratify_col\n",
    "    , name\n",
    "    , description\n",
    "    , predictor_id\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument | Type | Default | Description\n",
    "--- | --- | --- | ---\n",
    "**feature_ids** | list(int) | Required | Multiple `Feature.id`'s may be included to enable multi-modal (aka mixed data-type) analysis. All of these Features must have the same number of samples.\n",
    "**label_id** | int | None | The Label to be used as a target for supervised analysis. Must have the sample number of samples as the Features.\n",
    "**size_test** | float | None | Percent of samples to be placed into the test split. Must be `> 0.0` and `< 1.0`.\n",
    "**size_validation** | float | None | Percent of samples to be placed into the validation split. Must be `> 0.0` and `< 1.0`. If this is not None and used in combination with `fold_count`, then there will be 4 splits.\n",
    "**bin_count** | int | None | For continous stratification columns, how many bins (aka quantiles) should be used?\n",
    "**fold_count** | int | None | The number or cross-validation folds to generate. See [Cross-Validation](#5b.-Cross-Validation).\n",
    "**unsupervised_stratify_col** | str | None | Used during unsupervised analysis. Specify a column from the first Feature in feature_ids to use for stratification. For example, when forecasting, it may make sense to stratify by the day of the year.\n",
    "**name** | str | None | Used for versioning a pipeline (collection of inputs, label, and stratification). Two versions cannot have identical attributes.\n",
    "**description** | str | None | What is unique about this this pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `size_train = 1.00 - (size_test + size_validation)` the backend ensures that the sizes sum to 1.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *How does continuous binning work?* Reference the handy `Pandas.qcut()`  and the source code `pd.qcut(x=array_to_bin, q=bin_count, labels=False, duplicates='drop')` for more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8b. Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is triggered by `fold_count:int` during Splitset creation. Reference the [scikit-learn documentation](https://scikit-learn.org/stable/modules/cross_validation.html) to learn more about cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cross fold objects](../_static/images/api/cross_fold_objects.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the diagram above is a `Fold` object.\n",
    "\n",
    "Each green/blue box represents a bin of stratified samples. During preprocessing and training, we rotate which blue bin serves as the validation samples (`fold_validation`). The remaining green bins in the row serve as the training samples (`folds_train_combined`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we defined `fold_count=5`. What are the implications?\n",
    "- Creates 5 `Folds` related to a `Splitset`.\n",
    "- 5x more models will be trained for each experiment.\n",
    "- 5x more preprocessing and caching; the backend must preprocess each Fold separately to prevent data leakage by excluding `fold_validation` from the `fit`. Fits are saved to the Fold object as opposed to the Splitset object.\n",
    "- 5x more evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Disclaimer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> DO NOT use cross-validation unless your *(total sample count / fold_count)* still gives you an accurate representation of your entire sample population. If you are ignoring that advice and stretching to perform cross-validation, then at least ensure that *(total sample count / fold_count)* is evenly divisible. Folds naturally have fewer samples, so a handful of incorrect predictions have the potential to offset your aggregate metrics. Both of these tips help avoid poorly stratified/ undersized folds that seem to perform either too well (only most common label class present) or poorly (handful of samples and a few inaccurate prediction on an otherwise good model).\n",
    "> \n",
    "> Candidly, if you've ever performed cross-validation manually, let alone systematically, you'll know that, barring stratification of continuous labels, it's easy enough to construct the folds, but then it's a pain to generate performance metrics (e.g. `zero_division`, absent OHE classes) due to the absence of outlying classes and bins.  Time has been invested to handle these scenarios elegantly so that folds can be treated as first-class-citizens alongside splits. That being said, if you try to do something undersized like \"150 samples in their dataset and a `fold_count` > 3 with `unique_classes` > 4,\" then you may run into edge cases.\n",
    ">\n",
    "> Cross validation is only included in AIQC to allow best practices to be upheld and to show off the power of systematic preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Defining Architectures & Hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Define an `Algorithm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data has been prepared, we transition to the other half of the ORM where the focus is the logic that will be applied to that data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> An `Algorithm` is our ORM's codename for a machine learning model since *Model* is the most important *reserved word* when it comes to ORMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following attributes tell AIQC how to handle the Algorithm behind the scenes:\n",
    "\n",
    "* `library` - right now, only 'keras' is supported.\n",
    "\n",
    "  * Each library's model object and callbacks (history, early stopping) need to be handled differently.\n",
    "  \n",
    "  \n",
    "* `analysis_type` - right now, these types are supported:\n",
    "\n",
    "  * `'classification_multi'`, `'classification_binary'`, `'regression'`.\n",
    "  \n",
    "  * Used to determine which performance metrics to run.\n",
    "  \n",
    "  * Must be compatible with the type of label fed to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Algorithm` is composed of the functions:\n",
    "\n",
    "* `fn_build`.\n",
    "\n",
    "* `fn_lose` (optional, inferred).\n",
    "\n",
    "* `fn_optimize` (optional, inferred).\n",
    "\n",
    "* `fn_train`.\n",
    "\n",
    "* `fn_predict` (optional, inferred).\n",
    "\n",
    "> May provide overridable defaults for build and train in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can name the functions whatever you want, but do not change the predetermined arguments (e.g. `input_shape`,`**hp`, `model`, etc.) or their position.\n",
    "\n",
    "As we define these functions, we'll see that we can pass a dictionary of *hyperparameters* into these function using the `**hp` kwarg, and access them like so: `hp['<some_variable_name>']`. Later, we'll provide a list of values for each entry in the hyperparameters dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the modules that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Later, when running your `Job`'s, if you receive a \"module not found\" error, then you can try troubleshooting by importing that module directly within the function where it is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### └── `fn_build`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can build your topology however you like, just be sure to `return model`. Also, you don't have to use any of the hyperparameters (`**hp`) if you don't want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The automatically provided `features_shape` and `label_shape` are handy because:\n",
    "\n",
    "* The number of feature/ label columns is mutable due to encoders (e.g. OHE). \n",
    "\n",
    "* Shapes can be less obvious in multi-dimensional scenarios like colored images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_build(features_shape, label_shape, **hp):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(layers.Dense(units=hp['neuron_count'], input_shape=features_shape, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(units=hp['neuron_count'], activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(layers.Dense(units=label_shape[0], activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### └── `fn_lose` (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't just specify the loss function in our training loop because we will need it later on when it comes time to produce metrics about other splits/ folds.\n",
    "\n",
    "If you do not provide an `fn_lose` then one will be automatically selected for you based on the `Algorithm.analysis_type` you are conducting and the `Algorithm.library` you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_lose(**hp):\n",
    "    loser = tf.keras.losses.CategoricalCrossentropy()\n",
    "    return loser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### └── `fn_optimize` (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some deep learning libraries persist their model and optimizer separately during checkpoint/exporting. So `fn_optimize` provides an isolated way to access the optimizer. It also allows us to automatically set the optimizer.\n",
    "\n",
    "If you do not provide an `fn_optimize` then one will be automatically selected for you based on the `Algorithm.analysis_type` you are conducting and the `Algorithm.library` you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_optimize(**hp):\n",
    "    optimizer = tf.keras.optimizers.Adamax(learning_rate=0.01)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you want to define your own optimizer, then you should do so within this function, rather than relying on `model.compile(optimizer='<some_optimizer_name>'`. If you do not define an optimizer, then `Adamax` will be used by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### └── `fn_train`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `samples_train` - the appropriate data will be fed into the training cycle. For example, `Fold.samples[fold_index]['folds_train_combined']` or `Splitset.samples['train']`.\n",
    "\n",
    "* `samples_evaluate` - the appropriate data is made available for evaluation. For example, `Fold.samples[fold_index]['fold_validation']`, `Splitset.samples['validation']`, or `Splitset.samples['test']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_train(\n",
    "    model, loser, optimizer,\n",
    "    train_features, train_label,\n",
    "    eval_features, eval_label,\n",
    "    **hp\n",
    "):\n",
    "    model.compile(\n",
    "        loss        = loser\n",
    "        , optimizer = optimizer\n",
    "        , metrics   = ['accuracy']\n",
    "    )\n",
    "    model.fit(\n",
    "        train_features, train_label,\n",
    "        , validation_data = (eval_features, eval_label)\n",
    "        , verbose         = 0\n",
    "        , batch_size      = 3\n",
    "        , epochs          = hp['epoch_count']\n",
    "        , callbacks       = [tf.keras.callbacks.History()]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TensorFlow: either write a custom loop or use Keras.\n",
    "* PyTorch: either write a custom loop or use AIQC's `utils.pytorch.fit`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "model, history = fit(\n",
    "    model:object\n",
    "    , loser:object\n",
    "    , optimizer:object\n",
    "    \n",
    "    , train_features:object\n",
    "    , train_label:object\n",
    "    , eval_features:object\n",
    "    , eval_label:object\n",
    "    \n",
    "    , epochs:int              = 30\n",
    "    , batch_size:int          = 5\n",
    "    , enforce_sameSize:bool   = True\n",
    "    , allow_singleSample:bool = False\n",
    "    , metrics:list            = None\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Argument | Description |\n",
    "| --- | --- |\n",
    "| *enforce_sameSize* | removes the last batch if it is a different size |\n",
    "| *allow_singleSample* | errors if a batch only contains 1 sample, which often leads to layer errors during training |\n",
    "| *metrics* | instantiated `torchmetrics` classes e.g. `Accuracy` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Customizable history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the `Predictor.history` object is to record the training and evaluation metrics at the end of each epic so that they can be interpretted in the learning curve plots. Reference the [visualization](visualization.html) section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Keras*: any `metrics=[]` specified are automatically added to the `History` callback object.\n",
    "\n",
    "- *PyTorch*: users are responsible for calculating their own metrics (we recommend the `torchmetrics` package) and placing them into a `history` dictionary that mirrors the schema of the Keras history object. Reference the torch [examples](gallery/pytorch/multi_class.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The schema of this dictionary is as follows: `dict(*:ndarray, val_*=ndarray)`. For example, if you wanted to record the history of the 'loss' and 'accuracy' metrics manually for PyTorch, you would construct it like so:\n",
    "\n",
    "```python\n",
    "history = dict(\n",
    "    loss=ndarray, val_loss=ndarray,\n",
    "    accuracy=ndarray, val_accuracy=ndarray,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optional, callback to stop training early."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Early stopping* isn't just about efficiency in reducing the number of `epochs`. If you've specified 300 epochs, there's a chance your model catches on to the underlying patterns early, say around 75-125 epochs. At this point, there's also good chance what it learns in the remaining epochs will cause it to overfit on patterns that are specific to the training data, and thereby and lose it's simplicity/ generalizability.\n",
    "\n",
    "> The `val_` prefix refers to the evaluation samples.\n",
    ">\n",
    "> Remember, regression does not have accuracy metrics.\n",
    ">\n",
    "> `TrainingCallback.MetricCutoff` is a custom class we wrote to make *early stopping* easier, so you won't find information about it in the official Keras documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiqc.utils.tensorflow import TrainingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_train(model, loser, optimizer, samples_train, samples_evaluate, **hp):\n",
    "    model.compile(\n",
    "        loss = loser\n",
    "        , optimizer = optimizer\n",
    "        , metrics = ['accuracy']\n",
    "    )\n",
    "        \n",
    "    #Define one or more metrics to monitor.\n",
    "    metrics_cuttoffs = [\n",
    "        {\"metric\":\"val_accuracy\", \"cutoff\":0.96, \"above_or_below\":\"above\"},\n",
    "        {\"metric\":\"val_loss\", \"cutoff\":0.1, \"above_or_below\":\"below\"}\n",
    "    ]\n",
    "    cutoffs = TrainingCallback.MetricCutoff(metrics_cuttoffs)\n",
    "    # Remember to append `cutoffs` to the list of callbacks.\n",
    "    callbacks=[tf.keras.callbacks.History(), cutoffs]\n",
    "    \n",
    "    # No changes here.\n",
    "    model.fit(\n",
    "        samples_train[\"features\"]\n",
    "        , samples_train[\"labels\"]\n",
    "        , validation_data = (\n",
    "            samples_evaluate[\"features\"]\n",
    "            , samples_evaluate[\"labels\"]\n",
    "        )\n",
    "        , verbose = 0\n",
    "        , batch_size = 3\n",
    "        , epochs = hp['epoch_count']\n",
    "        , callbacks = callbacks\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### └── `fn_predict` (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fn_predict` will be generated for you automatically if set to `None`. The `analysis_type` and `library` of the Algorithm help determine how to handle the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) Regression default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_predict(model, samples_predict):\n",
    "    predictions = model.predict(samples_predict['features'])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) Classification binary default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All classification `predictions`, both mutliclass and binary, must be returned in ordinal format. \n",
    "\n",
    "> For most libraries, classification algorithms output *probabilities* as opposed to actual predictions when running `model.predict()`. We want to return both of these object `predictions, probabilities` (the order matters) to generate performance metrics behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_predict(model, samples_predict):\n",
    "    probabilities = model.predict(samples_predict['features'])\n",
    "    # This is the official keras replacement for binary classes `.predict_classes()`\n",
    "    # It returns one array per sample: `[[0][1][0][1]]` \n",
    "    predictions = (probabilities > 0.5).astype(\"int32\")\n",
    "    \n",
    "    return predictions, probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii) Classification multiclass default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_predict(model, samples_predict):\n",
    "    import numpy as np\n",
    "    probabilities = model.predict(samples_predict['features'])\n",
    "    # This is the official keras replacement for multiclass `.predict_classes()`\n",
    "    # It returns one ordinal array per sample: `[[0][2][1][2]]` \n",
    "    predictions = np.argmax(probabilities, axis=-1)\n",
    "    \n",
    "    return predictions, probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group the functions together in an `Algorithm`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = Algorithm.make(\n",
    "    library = \"keras\"\n",
    "    , analysis_type = \"classification_multi\"\n",
    "    , fn_build = fn_build\n",
    "    , fn_train = fn_train\n",
    "    , fn_optimize = fn_optimize # Optional\n",
    "    , fn_predict = fn_predict # Optional\n",
    "    , fn_lose = fn_lose # Optional\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <!> Remember to use `make` and not `create`. Deceptively,  `create` exists because it is a standard, built-in ORM method. However, it does so without any validation logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Combinations of hyperparameters with `Hyperparamset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters are fed into Algorithm functions.\n",
    "\n",
    "The `hyperparameters` below will be automatically fed into the functions above as `**kwargs` via the `**hp` argument we saw earlier.\n",
    "\n",
    "For example, wherever you see `hp['epoch_count']`, it will pull from the *key:value* pair `\"epoch_count\": [30, 60]` seen below. Where \"model A\" would have 30 epochs and \"model B\" would have 60 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = dict(\n",
    "    neuron_count    = [12]\n",
    "    , epoch_count   = [30, 60]\n",
    "    , learning_rate = [0.01, 0.03]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Selection Strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grid search strategy.\n",
    "\n",
    "By default AIQC will generate all possible combinations.\n",
    "\n",
    "> With enough practice, practitioners will get a feel for what parameters and topologies make sense so you'll rely on shotgun-style approaches less and less. If you limit your experiments to 1-2 parameters at a time then it's easy to see their effect as an *independent variable*. You should really start with high-level things such as topologies (# of layers, # neurons per layer) and batch size before moving on to tuning the intra-layer nuances (activation methods, weight initialization). You're essentially testing high/ medium/ low or default/ edge case scenarios for each parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random selection strategy.\n",
    "\n",
    "Testing many different combinations in your initial runs can be a good way to get a feel for the parameter space. Although if you are doing this you'll find that many of your combinations are a bit too similar. So randomly sampling (with replacement) a few of them is a less computationally expensive way to go about this.\n",
    "\n",
    "* `search_count:int` the fixed # of combinations to sample.\n",
    "\n",
    "* `search_percent:float` a % of combinations to sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bayesian selection strategy.\n",
    "\n",
    "\"TPE (Tree-structured Parzen Estimator)\" via `hyperopt` has been suggested as a future area to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparamset = Hyperparamset.from_algorithm(\n",
    "\talgorithm_id = algorithm.id\n",
    "\t, hyperparameters = hyperparameters\n",
    "    , search_count = None\n",
    "    , search_percent = None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Hyperparamcombo` objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each unique combination of hyperparameters is recorded as a `Hyperparamcombo`.\n",
    "\n",
    "Ultimately, a training `Job` is constructed for each unique combinanation of hyperparameters aka `Hyperparamcombo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparamset.hyperparamcombo_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neuron_count</td>\n",
       "      <td>12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>epoch_count</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>learning_rate</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           param  value\n",
       "0   neuron_count  12.00\n",
       "1    epoch_count  30.00\n",
       "2  learning_rate   0.01"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparamset.hyperparamcombos[0].get_hyperparameters(as_pandas=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. `Queue` of training `Jobs`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Queue` is the central object of the \"logic side\" of the ORM. It ties together everything we need for training and hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = Queue.from_algorithm(\n",
    "    algorithm_id = algorithm.id\n",
    "    , splitset_id = splitset.id\n",
    "    , hyperparamset_id = hyperparamset.id # Optional.\n",
    "    , repeat_count = 3\n",
    "    , permute_count = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `repeat_count:int=1` allows us to run the same `Job` multiple times. Normally, each `Job` has 1 `Predictor` associated with it upon completion. However, when `repeat_count` (> 1 of course) is used, a single `Job` will have multiple `Predictors`.\n",
    "\n",
    "> Due to the fact that training is a *nondeterministic* process, we get different weights each time we train a model, even if we use the same set of parameters. Perhaps you've have the right topology and parameters, but, this time around, the model just didn't recgonize the patterns. Similar to flipping a coin, there is a degree of chance in it, but the real trend averages out upon repetition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `hide_test:bool=False` excludes the test split from the performance metrics and visualizations. This avoids data leakage by forcing the user to make decisions based on the performance on their model on the training and evaluation samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `permute_count:int=3` controls the number of times each feature column is shuffled before it's impact on loss is compared to the baseline training loss before the median taken. Feature importance is calculated for each column of each `Feature.id`, except for `Feature.dataset.dataset_type=='image'`. If you have many columns and all you care about is the final prediction, then it may make sense to set `permute_count=0` because permutations are computationally expensive & difficult to parallelize.\n",
    "\n",
    "> `[training loss - (median loss of <5> permutations)]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) `Job` objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `Job` in the Queue represents a `Hyperparamcombo` that needs to be trained.\n",
    "\n",
    "If a `Splitset.fold_count` was specified, then: \n",
    "\n",
    "- The number of jobs = `hyperparamcombo_count` * `fold_count`.\n",
    "- Each Job will have a `Fold`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Executing `Jobs`.\n",
    "\n",
    "There are two ways to execute a Queue of Jobs:\n",
    "\n",
    "#### i) `queue.run_jobs()`\n",
    "\n",
    "* Jobs are simply ran on a loop on the main *Process*.\n",
    "\n",
    "* Stop the Jobs with a keyboard interrupt e.g. `ctrl+Z/D/C` in Python shell or `i,i` in Jupyter.\n",
    "\n",
    "* It is the more reliable approach on Win/Mac/Lin.\n",
    "\n",
    "* Although this locks your main process (can't write more code) while models train, you can still fire up a second shell session or notebook.\n",
    "\n",
    "* Prototype your training jobs in this method so that you can see any errors that arise in the console.\n",
    "\n",
    "\n",
    "#### ii) DEPRECATED - `queue.run_jobs(in_background=True)`.\n",
    "\n",
    "*Support for background processing has not been restored after decoupling the preprocessing pipelines from the Queue/Job logic.*\n",
    "\n",
    "* The Jobs loop is executed on a separate, parallel `multiprocessing.Process`\n",
    "\n",
    "* Stop the Jobs with `queue.stop_jobs()` (also deprecated), which kills the parallel *Process* unless it already failed.\n",
    "\n",
    "* The benefit is that you can continue to code while your models are trained. There is no performance boost.\n",
    "\n",
    "* On Mac and Linux (Unix), `'fork'` multiprocessing is used (`force=True`), which allows us to display the progress bar. FYI, even in 'fork' mode, Python multiprocessing is much more fragile in Python 3.8, which seems to be caused by how pickling is handled in passing variables to the child process.\n",
    "\n",
    "* On Windows, `'spawn'` multiprocessing is used, which requires polling:\n",
    "\n",
    "  * `queue.poll_statuses()`\n",
    "  \n",
    "  * `queue.poll_progress(raw:bool=False, loop:bool=False, loop_delay:int=3)` where `raw=True` is just a float, `loop=True` won't stop checking jobs until they are all complete, and `loop_delay=3` checks the progress every 3 seconds. \n",
    "  \n",
    "* Also, during stress tests, I observed that when running multiple queues at the same time, the SQLite database would lock when simultaneous writes were attempted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔮 Training Models 🔮: 100%|████████████████████████████████████████| 12/12 [00:49<00:00,  4.12s/it]\n"
     ]
    }
   ],
   "source": [
    "queue.run_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The queue is interuptable. You can stop the execution of a queue and resume it later.\n",
    "\n",
    "> This also comes in handy if either your machine or Python kernel either crashes or are interupted by accident. Whatever the reason, rest easy, just `run_jobs()` again to pick up where you left off. Be aware that the `tqdm` iteration time in the progress bar will be wrong because it will be divided by the jobs already ran."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing during Job is recorded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the execution of a Job, the LabelCoder and FeatureCoders(s) tied to the Label and Feature(s) of the Splitset used during training will record their `fit()`.\n",
    "\n",
    "- `FittedLabelCoder` for the LabelCoder used.\n",
    "  - `fitted_encoders:object` to store the `fit`.\n",
    "- `FittedEncoderset` for each Feature used.\n",
    "  - `fitted_encoders:list` to store the `fit`(s).\n",
    "\n",
    "This process is critical for:\n",
    "\n",
    "- `inverse_transform()`'ing aka decoding predictions.\n",
    "- Encoding new data during inference exactly the same was as the samples that the model was trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes a lot of joins to fetch the fitted encoders after the fact. So these methods are used behind the scenes to make it a bit easier:\n",
    "\n",
    "- `Predictor.get_fitted_encoders(job, label)`\n",
    "- `Predictor.get_fitted_LabelCoder(job, feature)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) `Predictors` are the trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `Job` trains a `Predictor`. The following attributes are automatically written to the `Predictor` after training.\n",
    "    \n",
    "* `model_file`: serialization varies for Keras and Pytorch deep learning framework.\n",
    "\n",
    "* `input_shapes`: used by `get_model()` during inference.\n",
    "\n",
    "* `history`: per epoch metrics recorded during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = queue.jobs[0].predictors[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x18227fe90>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is just a proxy that passes argments to `Hyperparamcombo.get_hyperparameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.get_hyperparameters(as_pandas=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) `Predictions` are the output of a Predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you feed samples through a Predictor, you get Predictions. During training, Predictions are automatically generated for every split/fold that was tied to the Queue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Attribute | Description |\n",
    "| --- | --- | \n",
    "| *predictions* | decoded predictions ndarray for per split/ fold/ inference |\n",
    "| *feature_importance* | importance of each column. only for training split/fold |\n",
    "| *probabilities* | prediction probabilities per split/ fold. `None` for regression. |\n",
    "| *metrics* | statistics for each split/fold that vary based on the analysis_type.  |\n",
    "| *metrics_aggregate* | average for each statistic across all splits/folds. |\n",
    "| *plot_data* | metrics reformatted for plot functions. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'accuracy': 0.9607843137254902,\n",
       "  'f1': 0.9607503607503607,\n",
       "  'loss': 0.08033698052167892,\n",
       "  'precision': 0.9618055555555555,\n",
       "  'recall': 0.9607843137254902,\n",
       "  'roc_auc': 0.9985582468281432},\n",
       " 'validation': {'accuracy': 0.9444444444444444,\n",
       "  'f1': 0.9440559440559441,\n",
       "  'loss': 0.13840313255786896,\n",
       "  'precision': 0.9523809523809523,\n",
       "  'recall': 0.9444444444444444,\n",
       "  'roc_auc': 1.0},\n",
       " 'test': {'accuracy': 0.9333333333333333,\n",
       "  'f1': 0.9333333333333333,\n",
       "  'loss': 0.1355634480714798,\n",
       "  'precision': 0.9333333333333333,\n",
       "  'recall': 0.9333333333333333,\n",
       "  'roc_auc': 0.9900000000000001}}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queue.jobs[0].predictors[0].predictions[0].metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Metrics & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on visualization of performance metrics, reference the [Visualization & Metrics](visualization.html) documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
