{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "comprehensive-globe",
   "metadata": {},
   "source": [
    "# TensorFlow2: Training Loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-radio",
   "metadata": {},
   "source": [
    "![gradient](../images/gradient_descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-armstrong",
   "metadata": {},
   "source": [
    "Although Keras is suitable for the vast majority of use cases, in the following scenarios, it may make sense to forgo `model.fit()` to manually define a training loop:\n",
    "\n",
    "- Maintaining legacy code and retraining old models.\n",
    "- Custom batch/ epoch operations like gradients and backpropagation.\n",
    "\n",
    "> Disclaimer; This notebook demonstrates how to manually define a training loop for queued tuning of a binary classification model. However, it is only included to prove that AIQC technically supports TensorFlow out-of-the-box with `analysis_type='keras'`, and to demonstrate how expert practicioners to do continue to use their favorite tools. We neither claim to be experts on the inner-workings of TensorFlow, nor do we intend to troubleshoot advanced methodologies for users that are in over their heads.\n",
    "\n",
    "Reference this repository for more TensorFlow cookbooks: \n",
    "> https://github.com/IvanBongiorni/TensorFlow2.0_Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "atlantic-alfred",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, PowerTransformer\n",
    "\n",
    "import aiqc\n",
    "from aiqc import datum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-hampton",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-hypothesis",
   "metadata": {},
   "source": [
    "## Example Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-textbook",
   "metadata": {},
   "source": [
    "Reference [Example Datasets](example_datasets.ipynb) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "round-adrian",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = datum.to_pandas('sonar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "completed-prefix",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>f</th>\n",
       "      <th>g</th>\n",
       "      <th>h</th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>...</th>\n",
       "      <th>az</th>\n",
       "      <th>ba</th>\n",
       "      <th>bb</th>\n",
       "      <th>bc</th>\n",
       "      <th>bd</th>\n",
       "      <th>be</th>\n",
       "      <th>bf</th>\n",
       "      <th>bg</th>\n",
       "      <th>bh</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        a       b       c       d       e       f       g       h       i  \\\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "\n",
       "        j  ...      az      ba      bb      bc      bd      be      bf  \\\n",
       "0  0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "1  0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "2  0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "3  0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "4  0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "\n",
       "       bg      bh  object  \n",
       "0  0.0090  0.0032       R  \n",
       "1  0.0052  0.0044       R  \n",
       "2  0.0095  0.0078       R  \n",
       "3  0.0040  0.0117       R  \n",
       "4  0.0107  0.0094       R  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-bumper",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-rebate",
   "metadata": {},
   "source": [
    "## a) High-Level API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-companion",
   "metadata": {},
   "source": [
    "Reference [High-Level API Docs](api_high_level.ipynb) for more information including how to work with non-tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "brave-meeting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "___/ featurecoder_index: 0 \\_________\n",
      "\n",
      "=> The column(s) below matched your filter(s) and were ran through a test-encoding successfully.\n",
      "\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'aa', 'ab', 'ac', 'ad', 'ae', 'af', 'ag', 'ah', 'ai', 'aj', 'ak', 'al', 'am', 'an', 'ao', 'ap', 'aq', 'ar', 'as', 'at', 'au', 'av', 'aw', 'ax', 'ay', 'az', 'ba', 'bb', 'bc', 'bd', 'be', 'bf', 'bg', 'bh']\n",
      "\n",
      "=> Done. All feature column(s) have encoder(s) associated with them.\n",
      "No more Featurecoders can be added to this Encoderset.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "splitset = aiqc.Pipeline.Tabular.make(\n",
    "    dataFrame_or_filePath = df\n",
    "    , label_column = 'object'\n",
    "    , size_test = 0.22\n",
    "    , size_validation = 0.12\n",
    "    , label_encoder = LabelBinarizer(sparse_output=False)\n",
    "    , feature_encoders = [{\n",
    "        \"sklearn_preprocess\": PowerTransformer(method='yeo-johnson', copy=False)\n",
    "        , \"dtypes\": ['float64']\n",
    "    }]\n",
    "    \n",
    "    , dtype = None\n",
    "    , features_excluded = None\n",
    "    , fold_count = None\n",
    "    , bin_count = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "undefined-editing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_build(features_shape, label_shape, **hp):\n",
    "    model = Sequential(name='Sonar')\n",
    "    model.add(Dense(hp['neuron_count'], activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dropout(0.30))\n",
    "    model.add(Dense(hp['neuron_count'], activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dropout(0.30))\n",
    "    model.add(Dense(hp['neuron_count'], activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(units=label_shape[0], activation='sigmoid', kernel_initializer='glorot_uniform'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "tutorial-tuesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_lose(**hp):\n",
    "\tloser = tf.losses.BinaryCrossentropy()\n",
    "\treturn loser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "leading-participant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_optimize(**hp):\n",
    "\toptimizer = tf.optimizers.Adamax()\n",
    "\treturn optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "thermal-principle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_train(model, loser, optimizer, samples_train, samples_evaluate, **hp):\n",
    "    batched_train_features, batched_train_labels = aiqc.tf_batcher(\n",
    "        features = samples_train['features']\n",
    "        , labels = samples_train['labels']\n",
    "        , batch_size = 5\n",
    "    )\n",
    "    \n",
    "    # Still necessary for saving entire model.\n",
    "    model.compile(loss=loser, optimizer=optimizer)\n",
    "    \n",
    "    ## --- Metrics ---\n",
    "    acc = tf.metrics.BinaryAccuracy()\n",
    "    # Mirrors `keras.model.History.history` object.\n",
    "    history = {\n",
    "        'loss':list(), 'accuracy': list(), \n",
    "        'val_loss':list(), 'val_accuracy':list()\n",
    "    }\n",
    "\n",
    "    ## --- Training loop ---\n",
    "    for epoch in range(hp['epochs']):\n",
    "        # --- Batch training ---\n",
    "        for i, batch in enumerate(batched_train_features):      \n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                batch_loss = loser(\n",
    "                    batched_train_labels[i],\n",
    "                    model(batched_train_features[i])\n",
    "                )\n",
    "            # Update weights based on the gradient of the loss function.\n",
    "            gradients = tape.gradient(batch_loss, model.trainable_variables)            \n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        ## --- Epoch metrics ---\n",
    "        # Overall performance on training data.\n",
    "        train_probability = model.predict(samples_train['features'])\n",
    "        train_loss = loser(samples_train['labels'], train_probability)\n",
    "        train_acc = acc(samples_train['labels'], train_probability)\n",
    "        history['loss'].append(float(train_loss))\n",
    "        history['accuracy'].append(float(train_acc))\n",
    "        # Performance on evaluation data.\n",
    "        eval_probability = model.predict(samples_evaluate['features'])\n",
    "        eval_loss = loser(samples_evaluate['labels'], eval_probability)\n",
    "        eval_acc = acc(samples_evaluate['labels'], eval_probability)\n",
    "        history['val_loss'].append(float(eval_loss))\n",
    "        history['val_accuracy'].append(float(eval_acc))\n",
    "    # Attach history to the model so we can return a single object.\n",
    "    model.history.history = history \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "through-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"neuron_count\": [25, 50]\n",
    "    , \"epochs\": [75, 150]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "current-livestock",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = aiqc.Experiment.make(\n",
    "    library = \"keras\"\n",
    "    , analysis_type = \"classification_binary\"\n",
    "    , fn_build = fn_build\n",
    "    , fn_train = fn_train\n",
    "    , fn_lose = fn_lose\n",
    "    , fn_optimize = fn_optimize\n",
    "    , splitset_id = splitset.id\n",
    "    , repeat_count = 1\n",
    "    , hide_test = False\n",
    "    , hyperparameters = hyperparameters\n",
    "\n",
    "    , fn_predict = None #automated\n",
    "    , foldset_id = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "federal-chemical",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ”® Training Models ðŸ”®: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:12<00:00, 33.19s/it]\n"
     ]
    }
   ],
   "source": [
    "queue.run_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-senate",
   "metadata": {},
   "source": [
    "For more information on visualization of performance metrics, reference the [Visualization & Metrics](visualization.html) documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-acceptance",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-cheese",
   "metadata": {},
   "source": [
    "## b) Low-Level API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-interstate",
   "metadata": {},
   "source": [
    "Reference [Low-Level API Docs](api_high_level.ipynb) for more information including how to work with non-tabular data and defining optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "herbal-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = aiqc.Dataset.Tabular.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "lightweight-royalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column = 'object'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "induced-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = dataset.make_label(columns=[label_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "similar-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelcoder = label.make_labelcoder(\n",
    "    sklearn_preprocess = LabelBinarizer(sparse_output=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "unsigned-stadium",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = dataset.make_feature(exclude_columns=[label_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "brown-circle",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoderset = feature.make_encoderset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "therapeutic-wisconsin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "___/ featurecoder_index: 0 \\_________\n",
      "\n",
      "=> The column(s) below matched your filter(s) and were ran through a test-encoding successfully.\n",
      "\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'aa', 'ab', 'ac', 'ad', 'ae', 'af', 'ag', 'ah', 'ai', 'aj', 'ak', 'al', 'am', 'an', 'ao', 'ap', 'aq', 'ar', 'as', 'at', 'au', 'av', 'aw', 'ax', 'ay', 'az', 'ba', 'bb', 'bc', 'bd', 'be', 'bf', 'bg', 'bh']\n",
      "\n",
      "=> Done. All feature column(s) have encoder(s) associated with them.\n",
      "No more Featurecoders can be added to this Encoderset.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurecoder_0 = encoderset.make_featurecoder(\n",
    "    sklearn_preprocess = PowerTransformer(method='yeo-johnson', copy=False)\n",
    "    , dtypes = ['float64']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "instrumental-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitset = aiqc.Splitset.make(\n",
    "    feature_ids = [feature.id]\n",
    "    , label_id = label.id\n",
    "    , size_test = 0.22\n",
    "    , size_validation = 0.12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "persistent-conspiracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_build(features_shape, label_shape, **hp):\n",
    "    model = Sequential(name='Sonar')\n",
    "    model.add(Dense(hp['neuron_count'], activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dropout(0.30))\n",
    "    model.add(Dense(hp['neuron_count'], activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dropout(0.30))\n",
    "    model.add(Dense(hp['neuron_count'], activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(units=label_shape[0], activation='sigmoid', kernel_initializer='glorot_uniform'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "regional-baking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_lose(**hp):\n",
    "\tloser = tf.losses.BinaryCrossentropy()\n",
    "\treturn loser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "biological-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_optimize(**hp):\n",
    "\toptimizer = tf.optimizers.Adamax()\n",
    "\treturn optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "returning-preview",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_train(model, loser, optimizer, samples_train, samples_evaluate, **hp):\n",
    "    batched_train_features, batched_train_labels = aiqc.tf_batcher(\n",
    "        features = samples_train['features']\n",
    "        , labels = samples_train['labels']\n",
    "        , batch_size = 5\n",
    "    )\n",
    "    \n",
    "    # Still necessary for saving entire model.\n",
    "    model.compile(loss=loser, optimizer=optimizer)\n",
    "    \n",
    "    ## --- Metrics ---\n",
    "    acc = tf.metrics.BinaryAccuracy()\n",
    "    # Mirrors `keras.model.History.history` object.\n",
    "    history = {\n",
    "        'loss':list(), 'accuracy': list(), \n",
    "        'val_loss':list(), 'val_accuracy':list()\n",
    "    }\n",
    "\n",
    "    ## --- Training loop ---\n",
    "    for epoch in range(hp['epochs']):\n",
    "        # --- Batch training ---\n",
    "        for i, batch in enumerate(batched_train_features):      \n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                batch_loss = loser(\n",
    "                    batched_train_labels[i],\n",
    "                    model(batched_train_features[i])\n",
    "                )\n",
    "            # Update weights based on the gradient of the loss function.\n",
    "            gradients = tape.gradient(batch_loss, model.trainable_variables)            \n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        ## --- Epoch metrics ---\n",
    "        # Overall performance on training data.\n",
    "        train_probability = model.predict(samples_train['features'])\n",
    "        train_loss = loser(samples_train['labels'], train_probability)\n",
    "        train_acc = acc(samples_train['labels'], train_probability)\n",
    "        history['loss'].append(float(train_loss))\n",
    "        history['accuracy'].append(float(train_acc))\n",
    "        # Performance on evaluation data.\n",
    "        eval_probability = model.predict(samples_evaluate['features'])\n",
    "        eval_loss = loser(samples_evaluate['labels'], eval_probability)\n",
    "        eval_acc = acc(samples_evaluate['labels'], eval_probability)\n",
    "        history['val_loss'].append(float(eval_loss))\n",
    "        history['val_accuracy'].append(float(eval_acc))\n",
    "    # Attach history to the model so we can return a single object.\n",
    "    model.history.history = history \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "typical-apple",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = aiqc.Algorithm.make(\n",
    "    library = \"keras\"\n",
    "    , analysis_type = \"classification_binary\"\n",
    "    , fn_build = fn_build\n",
    "    , fn_train = fn_train\n",
    "    , fn_lose = fn_lose\n",
    "    , fn_optimize = fn_optimize\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "immediate-venice",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"neuron_count\": [25, 50]\n",
    "    , \"epochs\": [75, 150]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "comfortable-booth",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"neuron_count\": [25, 50]\n",
    "    , \"epochs\": [75, 150]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "composite-dairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparamset = algorithm.make_hyperparamset(\n",
    "    hyperparameters = hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "wrapped-stone",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = algorithm.make_queue(\n",
    "    splitset_id = splitset.id\n",
    "    , hyperparamset_id = hyperparamset.id\n",
    "    , repeat_count = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "advisory-specification",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ”® Training Models ðŸ”®: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [04:19<00:00, 32.46s/it]\n"
     ]
    }
   ],
   "source": [
    "queue.run_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-cisco",
   "metadata": {},
   "source": [
    "For more information on visualization of performance metrics, reference the [Visualization & Metrics](visualization.html) documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
